@book{latex2e,
  author = {Leslie Lamport},
  year = {1994},
  title = {\LaTeX: a Document Preparation System},
  publisher = {Addison Wesley},
  address = {Massachusetts},
  edition = {2}
}

@article{knuth:1984,
  title={Literate Programming},
  author={Donald E. Knuth},
  journal={The Computer Journal},
  volume={27},
  number={2},
  pages={97--111},
  year={1984},
  publisher={Oxford University Press}
}

@article{TIGGEMANN200473,
title = {A word-stem completion task to assess implicit processing of appearance-related information},
journal = {Journal of Psychosomatic Research},
volume = {57},
number = {1},
pages = {73-78},
year = {2004},
issn = {0022-3999},
doi = {https://doi.org/10.1016/S0022-3999(03)00565-8},
url = {https://www.sciencedirect.com/science/article/pii/S0022399903005658},
author = {Marika Tiggemann and Duane Hargreaves and Janet Polivy and Traci McFarlane},
keywords = {Appearance schemas, Word stems, Media, Cognitive processing},
abstract = {Objective
This paper reports on the development and utility of a new implicit measure of appearance-related information processing.
Methods
A 20-item word-stem completion task was constructed, in which each word stem could be completed with either an appearance-related word or at least one non-appearance alternative. The measure was tested in four different experiments, most investigating the impact of acute exposure to media-portrayed thin idealised female images.
Results
Exposure to media images or other appearance-related material led to the generation of more appearance- or weight-related words in both female and male samples.
Conclusion
It was concluded that the word-stem task has empirical utility as a simple, self-paced and sensitive outcome measure in experimental studies of media exposure. We conceptualise the word-stem task as a measure of appearance- and weight-schema activation.}
}

@Article{pmid26321987,
   Author="Soler, M. J.  and Dasí, C.  and Ruiz, J. C. ",
   Title="{{P}riming in word stem completion tasks: comparison with previous results in word fragment completion tasks}",
   Journal="Front Psychol",
   Year="2015",
   Volume="6",
   Pages="1172"
}

@Article{pmid17065885,
   Author="Stonell, C. A.  and Leslie, K.  and He, C.  and Lee, L. ",
   Title="{{N}o sex differences in memory formation during general anesthesia}",
   Journal="Anesthesiology",
   Year="2006",
   Volume="105",
   Number="5",
   Pages="920--926",
   Month="Nov"
}

@article{porterstemmer,
author = {Porter, M.F.},
year = {2006},
month = {07},
pages = {130-137},
title = {An algoritm for suffix stripping},
volume = {14},
journal = {Program},
doi = {10.1108/00330330610681286}
}

@article{Feinerer2010,
title = {Analysis and algorithms for stemming inversion},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year = {2010},
volume = {6458 LNCS},
pages = {290-299},
author = {Feinerer, I.}
}

@article{stemcompletionpackage,
title = {stemCompletion: Complete Stems},
year = {2010},
author = {Feinerer, I.},
URL = {https://www.rdocumentation.org/packages/tm/versions/0.7-8/topics/stemCompletion}
}

@article{UyarStemming,
author = {Ahmet Uyar},
title ={Google stemming mechanisms},
journal = {Journal of Information Science},
volume = {35},
number = {5},
pages = {499-514},
year = {2009},
doi = {10.1177/1363459309336801},
URL = {https://doi.org/10.1177/1363459309336801},
eprint = {https://doi.org/10.1177/1363459309336801},
abstract = { In this study we investigated the stemming mechanisms of Google. We used its web interface and submitted many queries via a program. Stemming is the process of correlating morphologically similar words with one another. Search engines use stemming to match documents having one form of a word with queries having another form of the same word. We investigated the stemming mechanism of Google for three classes of words: singulars/plurals, combined words, and verbs with many postfixes. Our results indicate that Google uses a document-based algorithm for stemming. It evaluates each document separately and makes a decision to index or not for the conflated forms of the words it has. It indexes documents only for word forms that are semantically strongly correlated. While it indexes documents for singulars and plurals frequently, it rarely indexes documents for word forms with the postfixes of -able or -tively. }
}

@article{ReutersDataset,
author = {D Lewis},
title ={Reuters-21578 text categorization test collection},
year = {1997},
URL = {http://www.daviddlewis.com/resources/testcollections/reuters21578/}
}

@article{gutenberg,
author = {Project Gutenberg},
title ={Project Gutenberg is a library of over 60,000 free eBooks},
year = {2021},
URL = {www.gutenberg.org}
}

@article{wordlength,
author = {Bochkarev, Vladimir and Shevlyakova, Anna and Solovyev, Valery},
year = {2012},
month = {08},
pages = {153-175},
title = {Average word length dynamics as indicator of cultural changes in society},
volume = {14},
journal = {Social Evolution and History}
}

@article{vlado,
author = {Vlado Keselj},
year = {2021},
title = {Lecture Notes: CSCI 4152/6509 — Natural Language Processing},
URL = {https://web.cs.dal.ca/~vlado/csci6509/coursecalendar.html},
}

@book{nlpbook,
author = {Jurafsky, Daniel and Martin, James},
year = {2008},
month = {02},
title = {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
}

@misc{transformerOrig,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{lstm,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}

@misc{bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gpt,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}